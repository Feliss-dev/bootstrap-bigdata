spring.application.name=backend

server.port=8080

# HDFS Configuration
app.hdfsUri=hdfs://localhost:9000
app.sparkMaster=spark://localhost:7077
app.spark.driverHost=localhost
app.spark.driverMemory=2g
app.spark.executorMemory=2g
app.spark.executorInstances=2
app.paths.inputDir=/data
app.paths.outputDir=/output

# Spark Configuration
spark.driver.host=localhost
spark.driver.bindAddress=localhost
spark.driver.port=0
spark.sql.execution.arrow.pyspark.enabled=false
spark.sql.adaptive.localShuffleReader.enabled=true
spark.serializer=org.apache.spark.serializer.KryoSerializer

# HDFS Client Configuration - giáº£m replication requirements
spark.hadoop.fs.defaultFS=hdfs://localhost:9000
spark.hadoop.dfs.client.use.datanode.hostname=false
spark.hadoop.dfs.datanode.use.datanode.hostname=false
spark.hadoop.dfs.replication=1
spark.hadoop.dfs.namenode.replication.min=1
spark.hadoop.dfs.client.socket-timeout=60000
spark.hadoop.dfs.client.max.block.acquire.failures=3

# HTTP Configuration
spring.servlet.multipart.max-file-size=500MB
spring.servlet.multipart.max-request-size=500MB
spring.http.encoding.charset=UTF-8
spring.http.encoding.enabled=true
spring.http.encoding.force=true

# Logging
logging.level.org.apache.spark=WARN
logging.level.org.apache.hadoop=WARN
logging.level.org.eclipse.jetty=WARN
logging.level.io.netty=WARN
logging.level.org.apache.hadoop.hdfs=INFO

